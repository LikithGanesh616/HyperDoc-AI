# HyperDoc AI

A full-stack Retrieval-Augmented Generation (RAG) demo: ingest PDFs, index them in Qdrant, and chat in real time with a Llama 3 model.

---

## ğŸš€ Features

- **PDF Ingestion**  
  Upload a PDF â†’ chunk â†’ embed â†’ index in Qdrant  
- **Vector Search**  
  intfloat/e5-large-v2 embeddings + Qdrant  
- **Generative AI**  
  Llama-3-8B GGUF via `llama-cpp` (streaming SSE)  
- **Modern Web UI**  
  Next.js 15 + Tailwind CSS, dark mode, Instagram-style chat bubbles  
- **MLOps Foundations**  
  - Docker Compose for Qdrant  
  - Model versioned on Hugging Face Hub  
  - Env-driven config (`LLAMA_PATH`, `QDRANT_URL`)

---

## ğŸ— Architecture

```
Browser (React + Next.js)
    â†•ï¸ SSE / REST
Next.js Frontend        FastAPI Backend
(TS + Tailwind)         (Python + llama-cpp)
    â†•ï¸ HTTP
  Qdrant Vector Store
```

---

## âš™ï¸ Quickstart

### Prerequisites

- Python 3.11+ (pyenv recommended)  
- Node.js v18+ & pnpm  
- Docker & Docker Compose  

### 1. Clone & Install

```bash
git clone https://github.com/your-org/HyperDoc-AI.git
cd HyperDoc-AI
```

#### Backend

```bash
cd backend
pyenv virtualenv 3.11.8 hyperdoc-ai
pyenv activate hyperdoc-ai
pip install -r requirements.txt
```

#### Frontend

```bash
cd ../frontend
pnpm install
```

### 2. Launch Qdrant

```bash
docker compose -f ../infra/docker-compose.dev.yml up -d --build
curl http://localhost:6333/collections
```

### 3. Run Services

Open **two** shells:

- **Backend**
  ```bash
  cd HyperDoc-AI/backend
  export LLAMA_PATH="app/models/Meta-Llama-3-8B.Q4_K_M.gguf"
  uvicorn app.main:app --reload --port 8001
  ```
- **Frontend**
  ```bash
  cd HyperDoc-AI/frontend
  pnpm dev
  ```

### 4. Try It

1. Visit `http://localhost:3000`  
2. Upload a PDF â†’ wait for â€œDocument ingested!â€  
3. Ask questions â†’ see streaming AI responses  

---

## ğŸ“‚ Project Layout

```
HyperDoc-AI/
â”‚
â”œâ”€ backend/
â”‚   â”œâ”€ app/
â”‚   â”‚   â”œâ”€ main.py
â”‚   â”‚   â”œâ”€ pdf_utils.py
â”‚   â”‚   â”œâ”€ embed_utils.py
â”‚   â”‚   â””â”€ rag_utils.py
â”‚   â””â”€ requirements.txt
â”‚
â”œâ”€ frontend/
â”‚   â”œâ”€ src/
â”‚   â”‚   â”œâ”€ app/
â”‚   â”‚   â”‚   â”œâ”€ layout.tsx
â”‚   â”‚   â”‚   â”œâ”€ page.tsx
â”‚   â”‚   â”‚   â””â”€ globals.css
â”‚   â”‚   â”œâ”€ components/
â”‚   â”‚   â”‚   â”œâ”€ Sidebar.tsx
â”‚   â”‚   â”‚   â”œâ”€ SearchBar.tsx
â”‚   â”‚   â”‚   â”œâ”€ UploadForm.tsx
â”‚   â”‚   â”‚   â””â”€ ChatBox.tsx
â”‚   â”‚   â””â”€ lib/
â”‚   â”‚       â””â”€ sse.ts
â”‚   â”œâ”€ tailwind.config.js
â”‚   â”œâ”€ postcss.config.js
â”‚   â””â”€ package.json
â”‚
â””â”€ infra/
    â””â”€ docker-compose.dev.yml
```



## ğŸ’¼ Resume Highlights

- Retrieval-Augmented Generation: PDF â†’ embeddings â†’ LLM  
- Vector embeddings + Qdrant search  
- FastAPI SSE streaming endpoints  
- React/Next.js + Tailwind dark-mode UI  
- Dockerized Qdrant, HF Hub model versioning, env-driven configs  

---

Made with â¤ï¸ by Likith Sompalli

"""
RAG utilities for HyperDoc AI
–––––––––––––––––––––––––––––
* Embeds queries with e5-large-v2
* Retrieves top-K chunks from Qdrant
* Generates answers with Llama-3-8B (GGUF via llama-cpp) – single + streaming
"""

from qdrant_client import QdrantClient
from sentence_transformers import SentenceTransformer
from huggingface_hub import hf_hub_download
from llama_cpp import Llama
import pathlib, os

# ──────────────────────────────────────────────────────────────────────────
# Model & retrieval settings
# ──────────────────────────────────────────────────────────────────────────
MODEL_PATH = os.getenv("LLAMA_PATH") or hf_hub_download(
    repo_id="MoriartyE/hyperdoc-llama3-8b",
    filename="Meta-Llama-3-8B.Q4_K_M.gguf",          # ← new file
    local_dir=str(pathlib.Path(__file__).parent / "models"),
    resume_download=True,
)

EMBED_MODEL = "intfloat/e5-large-v2"
COLLECTION  = "hyperdoc_chunks"
TOP_K       = 10            # more context chunks helps base models
STOP_TOKENS = ["\n###", "</s>"]
GEN_KWARGS  = dict(
    max_tokens=512,
    temperature=0.2,
    top_p=0.9,
    repeat_penalty=1.05,
    stop=STOP_TOKENS,
)

PROMPT_TEMPLATE = """You are HyperDoc AI.
Your job is to answer or summarise using ONLY the information inside <context>.
• If the answer truly cannot be found, reply exactly: "I don't know."

<context>
{context}
</context>

### Task
{question}

### Answer
"""

# ──────────────────────────────────────────────────────────────────────────
# Instantiate heavy objects once
# ──────────────────────────────────────────────────────────────────────────
_embedder = SentenceTransformer(EMBED_MODEL)
_qdrant   = QdrantClient(url=os.getenv("QDRANT_URL", "http://localhost:6333"))
_llm      = Llama(model_path=MODEL_PATH, n_ctx=8192, n_threads=8)


# ──────────────────────────────────────────────────────────────────────────
# Helper: single-shot answer
# ──────────────────────────────────────────────────────────────────────────
def answer(question: str) -> str:
    q_vec = _embedder.encode([f"query: {question}"])[0]
    hits  = _qdrant.search(
        collection_name=COLLECTION,
        query_vector=q_vec.tolist(),
        limit=TOP_K,
    )
    context = "\n\n".join(
        f"[{i+1}] {h.payload['text']}" for i, h in enumerate(hits)
    )
    prompt = PROMPT_TEMPLATE.format(context=context, question=question)
    out = _llm(prompt, **GEN_KWARGS)
    return out["choices"][0]["text"].strip()


# ──────────────────────────────────────────────────────────────────────────
# Helper: streaming answer (Server-Sent Events)
# ──────────────────────────────────────────────────────────────────────────
def stream_answer(question: str):
    """
    Yields token chunks as they are generated by llama-cpp
    so the front-end can display them in real time.
    """
    q_vec = _embedder.encode([f"query: {question}"])[0]
    hits  = _qdrant.search(
        collection_name=COLLECTION,
        query_vector=q_vec.tolist(),
        limit=TOP_K,
    )
    context = "\n\n".join(
        f"[{i+1}] {h.payload['text']}" for i, h in enumerate(hits)
    )
    prompt = PROMPT_TEMPLATE.format(context=context, question=question)

    for chunk in _llm(prompt, stream=True, **GEN_KWARGS):
        yield chunk["choices"][0]["text"]